{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8b770dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c5541f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece9495e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = [\n",
    "    'javascript',\n",
    "    'python',\n",
    "    'java',\n",
    "    'c_sharp',\n",
    "    'php',\n",
    "    'android',\n",
    "    'html',\n",
    "    'jquery',\n",
    "    'c_plus_plus',\n",
    "    'css',\n",
    "    'ios',\n",
    "    'sql',\n",
    "    'mysql',\n",
    "    'r',\n",
    "    'reactjs',\n",
    "    'node_js',\n",
    "    'arrays',\n",
    "    'c',\n",
    "    'asp_net',\n",
    "    'json'\n",
    "]\n",
    "\n",
    "X_tf_idf = pd.read_csv(\"X_tf_idf.csv\")\n",
    "X_word2vec = pd.read_csv(\"X_word2vec.csv\")\n",
    "X_bert = pd.read_csv(\"X_bert.csv\")\n",
    "X_use = pd.read_csv(\"X_use.csv\")\n",
    "\n",
    "df = pd.read_csv(\"Question Extraction V05 year.csv\")\n",
    "\n",
    "y = df[tags]\n",
    "\n",
    "title_vectorizer = pickle.load(open(\"title_vectorizer.sav\", \"rb\"))\n",
    "body_vectorizer = pickle.load(open(\"body_vectorizer.sav\", \"rb\"))\n",
    "\n",
    "\n",
    "# We need this function to evaluate data drift.\n",
    "def get_X_tf_idf(df, title_vectorizer, body_vectorizer):\n",
    "    cleaned_title = df['Title'].apply(clean_for_tf_idf)\n",
    "    title_tf_idf_features = title_vectorizer.transform(cleaned_title)\n",
    "    \n",
    "    title_tf_idf_features_df = pd.DataFrame(\n",
    "        data=title_tf_idf_features.toarray(),\n",
    "        columns=['title_' + feature_name for feature_name in title_vectorizer.get_feature_names_out()]\n",
    "    )\n",
    "    \n",
    "    cleaned_body = df['Body'].apply(clean_for_tf_idf)\n",
    "    body_tf_idf_features = body_vectorizer.transform(cleaned_body)\n",
    "\n",
    "    body_tf_idf_features_df = pd.DataFrame(\n",
    "        data=body_tf_idf_features.toarray(),\n",
    "        columns=['body_' + feature_name for feature_name in body_vectorizer.get_feature_names_out()]\n",
    "    )\n",
    "\n",
    "    X_tf_idf = pd.concat(\n",
    "        [\n",
    "            title_tf_idf_features_df,\n",
    "            body_tf_idf_features_df\n",
    "        ],\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    return X_tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9887c97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = .2\n",
    "\n",
    "X_tf_idf_train,   X_tf_idf_test,   _,       __     = train_test_split(X_tf_idf,   y, test_size=test_size, random_state=0)\n",
    "X_word2vec_train, X_word2vec_test, _,       __     = train_test_split(X_word2vec, y, test_size=test_size, random_state=0)\n",
    "X_bert_train,     X_bert_test,     _,       __     = train_test_split(X_bert,     y, test_size=test_size, random_state=0)\n",
    "X_use_train,      X_use_test,      y_train, y_test = train_test_split(X_use,      y, test_size=test_size, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea80985",
   "metadata": {},
   "outputs": [],
   "source": [
    "URI = \"http://127.0.0.1:8080\"\n",
    "\n",
    "client = mlflow.MlflowClient(tracking_uri=URI)\n",
    "\n",
    "# Provide an Experiment description that will appear in the UI\n",
    "experiment_description = (\n",
    "    \"\"\"Tag prediction V21. Comparing embeddings: tf-idf, Word2Vec, BERT, USE.\n",
    "    Comparing models: Logistic Regression, Random Forest, XGBoost.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Provide searchable tabs that define characteristics of the Runs\n",
    "# that will be in this Experiment\n",
    "experiment_tags = {\n",
    "    \"project_name\": \"stackoverflow_tagging\",\n",
    "    \"mlflow.note.content\": experiment_description\n",
    "}\n",
    "\n",
    "# Create the Experiment, providing a unique name\n",
    "experiment_name = \"tag_prediction_v21\"\n",
    "if not client.get_experiment_by_name(experiment_name):\n",
    "    client.create_experiment(name=experiment_name, tags=experiment_tags)\n",
    "\n",
    "mlflow.set_tracking_uri(URI)\n",
    "\n",
    "# Sets the current active experiment and\n",
    "# returns the Experiment metadata\n",
    "experiment = mlflow.set_experiment(experiment_name)\n",
    "\n",
    "# Define a run name for this iteration of training.\n",
    "# If this is not set, a unique name will be auto-generated for your run.\n",
    "\n",
    "# Define an artifact path that the model will be saved to.\n",
    "artifact_path = \"tag_prediction_21_artifact_path\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1585a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for embeddings_name in ['tf_idf', 'word2vec', 'bert', 'use']:\n",
    "    for model_name in ['logistic_regression', 'random_forest', 'xgboost']:\n",
    "                \n",
    "        run_name = embeddings_name + '_default_' + model_name\n",
    "        \n",
    "        params = {'embeddings_name': embeddings_name, 'model_name': model_name}\n",
    "\n",
    "        if embeddings_name == 'tf_idf':\n",
    "            X_train, X_test = X_tf_idf_train, X_tf_idf_test\n",
    "        elif embeddings_name == 'word2vec':\n",
    "            X_train, X_test = X_word2vec_train, X_word2vec_test\n",
    "        elif embeddings_name == 'bert':\n",
    "            X_train, X_test = X_bert_train, X_bert_test\n",
    "        else:\n",
    "            X_train, X_test = X_use_train, X_use_test\n",
    "\n",
    "        if model_name == 'logistic_regression':\n",
    "            model = OneVsRestClassifier(LogisticRegression(random_state=0))\n",
    "        elif model_name == 'random_forest':\n",
    "            model = RandomForestClassifier(random_state=0)\n",
    "        else:\n",
    "            model = XGBClassifier(random_state=0)\n",
    "        \n",
    "        fit_start = time.time()\n",
    "        model.fit(X_train, y_train)\n",
    "        fit_duration = time.time() - fit_start\n",
    "        \n",
    "        pred_start = time.time()\n",
    "        y_pred = model.predict(X_test)\n",
    "        pred_duration = time.time() - pred_start\n",
    "        \n",
    "        f1 = f1_score(y_test, y_pred, average='micro')\n",
    "        precision = precision_score(y_test, y_pred, average='micro')\n",
    "        recall = recall_score(y_test, y_pred, average='micro')\n",
    "        \n",
    "        metrics = {\n",
    "            'fit_duration': fit_duration, 'pred_duration': pred_duration,\n",
    "            'f1': f1, 'precision': precision, 'recall': recall\n",
    "        }\n",
    "        \n",
    "        with mlflow.start_run(run_name=run_name) as run:\n",
    "            mlflow.log_params(params)\n",
    "            mlflow.log_metrics(metrics)\n",
    "            mlflow.sklearn.log_model(\n",
    "                sk_model=model, input_example=X_test, artifact_path=artifact_path\n",
    "            )\n",
    "        print(f\"run name: {run_name}.\")\n",
    "        print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8f09ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.search_runs(\n",
    "    order_by=[\"metrics.f1 DESC\"],\n",
    "    search_all_experiments=True\n",
    ").iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3022c328",
   "metadata": {},
   "source": [
    "# Model fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d658c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to fine-tune XGBoost but it takes too long to run as is,\n",
    "# because X has 4,000 features. We want to decrease the number of features.\n",
    "# For that we look at the most important features, thanks to the\n",
    "# Logistic Regression model.\n",
    "# For each tag, we keep the 20 features with highest coefficient.\n",
    "# This will give us a smaller X.\n",
    "# We'll then be able to fine-tune XGBoost on that smaller X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab64e57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2e8bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model_artifact_uri = (\n",
    "    mlflow.search_runs(\n",
    "        filter_string=\"params.model_name = 'logistic_regression'\",\n",
    "        order_by=[\"metrics.f1 DESC\"],\n",
    "        search_all_experiments=True\n",
    "    ).iloc[0]['artifact_uri']\n",
    ")\n",
    "\n",
    "path = lr_model_artifact_uri + '/' + artifact_path\n",
    "\n",
    "lr_model = mlflow.sklearn.load_model(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73648e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_important_features(lr_model, X, tags=tags, n_features_per_tag=20):\n",
    "    most_important_features = set()\n",
    "    for i, tag in enumerate(tags):\n",
    "        coefs = lr_model.estimators_[i].coef_[0]\n",
    "        feature_importance_for_that_tag = pd.Series(\n",
    "            data=coefs,\n",
    "            index=X.columns\n",
    "        )\n",
    "        \n",
    "        important_features_for_that_tag = set(feature_importance_for_that_tag.sort_values(ascending=False).head(n_features_per_tag).index)\n",
    "        most_important_features = most_important_features.union(important_features_for_that_tag)\n",
    "    most_important_features = list(most_important_features)\n",
    "    return most_important_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c64df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_important_features = get_most_important_features(\n",
    "    lr_model=lr_model,\n",
    "    X=X_tf_idf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ffb7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tf_idf_20_train = X_tf_idf_train[most_important_features]\n",
    "X_tf_idf_20_test = X_tf_idf_test[most_important_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ab6900",
   "metadata": {},
   "outputs": [],
   "source": [
    "for embeddings_name in ['tf_idf_top_20']:\n",
    "    for model_name in ['xgboost']:\n",
    "        for eta in [.1, .3]:\n",
    "            for colsample_bytree in [.5, 1]:\n",
    "                for max_depth in [5, 6, 7]:\n",
    "                            \n",
    "                    run_name = (\n",
    "                        embeddings_name + '_' + model_name + '_eta_' + str(eta) +\n",
    "                        '_colsample_bytree_' + str(colsample_bytree) + '_max_depth_' + str(max_depth)\n",
    "                    )\n",
    "                    \n",
    "                    params = {\n",
    "                        'embeddings_name': embeddings_name, 'model_name': model_name,\n",
    "                        'eta': eta, 'colsample_bytree': colsample_bytree, 'max_depth': max_depth\n",
    "                    }\n",
    "            \n",
    "                    X_train, X_test = X_tf_idf_20_train, X_tf_idf_20_test\n",
    "            \n",
    "                    model = XGBClassifier(\n",
    "                        eta=eta, colsample_bytree=colsample_bytree, max_depth=max_depth,\n",
    "                        random_state=0\n",
    "                    )\n",
    "                    \n",
    "                    fit_start = time.time()\n",
    "                    model.fit(X_train, y_train)\n",
    "                    fit_duration = time.time() - fit_start\n",
    "                    \n",
    "                    pred_start = time.time()\n",
    "                    y_pred = model.predict(X_test)\n",
    "                    pred_duration = time.time() - pred_start\n",
    "                    \n",
    "                    f1 = f1_score(y_test, y_pred, average='micro')\n",
    "                    precision = precision_score(y_test, y_pred, average='micro')\n",
    "                    recall = recall_score(y_test, y_pred, average='micro')\n",
    "                    \n",
    "                    metrics = {\n",
    "                        'fit_duration': fit_duration, 'pred_duration': pred_duration,\n",
    "                        'f1': f1, 'precision': precision, 'recall': recall\n",
    "                    }\n",
    "                    \n",
    "                    with mlflow.start_run(run_name=run_name) as run:\n",
    "                        mlflow.log_params(params)\n",
    "                        mlflow.log_metrics(metrics)\n",
    "                        mlflow.sklearn.log_model(\n",
    "                            sk_model=model, input_example=X_test, artifact_path=artifact_path\n",
    "                        )\n",
    "                    duration = time.time() - start\n",
    "                    print(f\"run name: {run_name}.\")\n",
    "                    print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430246c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.search_runs(\n",
    "    order_by=[\"metrics.f1 DESC\"],\n",
    "    search_all_experiments=True\n",
    ").iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e0dbb5",
   "metadata": {},
   "source": [
    "# Model for tag prediction app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1442b72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're looking for a light, robust and high-performance model for the tag prediction app.\n",
    "\n",
    "# Fine-tuning Logistic Regression on full tf_idf embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d11330f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for embeddings_name in ['tf_idf']:\n",
    "    for model_name in ['logistic_regression']:\n",
    "        for penalty in ['l2', 'none']:\n",
    "                            \n",
    "                    run_name = (\n",
    "                        embeddings_name + '_' + model_name + '_penalty_' + penalty\n",
    "                    )\n",
    "                    \n",
    "                    params = {\n",
    "                        'embeddings_name': embeddings_name, 'model_name': model_name,\n",
    "                        'penalty': penalty\n",
    "                    }\n",
    "            \n",
    "                    X_train, X_test = X_tf_idf_train, X_tf_idf_test\n",
    "            \n",
    "                    model = OneVsRestClassifier(LogisticRegression(penalty=penalty, random_state=0))\n",
    "                    \n",
    "                    fit_start = time.time()\n",
    "                    model.fit(X_train, y_train)\n",
    "                    fit_duration = time.time() - fit_start\n",
    "                    \n",
    "                    pred_start = time.time()\n",
    "                    y_pred = model.predict(X_test)\n",
    "                    pred_duration = time.time() - pred_start\n",
    "                    \n",
    "                    f1 = f1_score(y_test, y_pred, average='micro')\n",
    "                    precision = precision_score(y_test, y_pred, average='micro')\n",
    "                    recall = recall_score(y_test, y_pred, average='micro')\n",
    "                    \n",
    "                    metrics = {\n",
    "                        'fit_duration': fit_duration, 'pred_duration': pred_duration,\n",
    "                        'f1': f1, 'precision': precision, 'recall': recall\n",
    "                    }\n",
    "                    \n",
    "                    with mlflow.start_run(run_name=run_name) as run:\n",
    "                        mlflow.log_params(params)\n",
    "                        mlflow.log_metrics(metrics)\n",
    "                        mlflow.sklearn.log_model(\n",
    "                            sk_model=model, input_example=X_test, artifact_path=artifact_path\n",
    "                        )\n",
    "                    duration = time.time() - start\n",
    "                    print(f\"run name: {run_name}.\")\n",
    "                    print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6c8692",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.search_runs(\n",
    "    filter_string=\"params.model_name = 'logistic_regression'\",\n",
    "    order_by=[\"metrics.f1 DESC\"],\n",
    "    search_all_experiments=True\n",
    ").iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3650ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_lr_model_artifact_uri = (\n",
    "    mlflow.search_runs(\n",
    "        filter_string=\"params.model_name = 'logistic_regression'\",\n",
    "        order_by=[\"metrics.f1 DESC\"],\n",
    "        search_all_experiments=True\n",
    "    ).iloc[0]['artifact_uri']\n",
    ")\n",
    "\n",
    "path = best_lr_model_artifact_uri + '/' + artifact_path\n",
    "\n",
    "best_lr_model = mlflow.sklearn.load_model(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc71626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model as Pickle file to use it in the tag prediction app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6f17d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(best_lr_model, open('tag_predictor.sav', \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a817ee8",
   "metadata": {},
   "source": [
    "# Evaluating model drift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db64870f",
   "metadata": {},
   "outputs": [],
   "source": [
    "year_range = range(2008, 2024)\n",
    "\n",
    "data_drift_Xs_dict = {}\n",
    "for year in year_range:\n",
    "    year_data_drift_df = df.loc[df['year'] == year]\n",
    "    \n",
    "    year_data_drift_X = get_X_tf_idf(\n",
    "        year_data_drift_df, title_vectorizer, body_vectorizer\n",
    "    )\n",
    "    data_drift_Xs_dict.update({year: year_data_drift_X})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0acd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "for embeddings_name in ['tf_idf']:\n",
    "    for model_name in ['logistic_regression']:\n",
    "        for year in year_range:\n",
    "            run_name = (\n",
    "                embeddings_name + '_best_' + model_name\n",
    "                + '_year_' + str(year)\n",
    "            )\n",
    "            \n",
    "            params = {\n",
    "                'embeddings_name': embeddings_name, 'model_name': model_name,\n",
    "                'year': year\n",
    "            }\n",
    "            \n",
    "            X_test = data_drift_Xs_dict[year]\n",
    "            \n",
    "            model = best_lr_model\n",
    "            \n",
    "            pred_start = time.time()\n",
    "            y_pred = model.predict(X_test)\n",
    "            pred_duration = time.time() - pred_start\n",
    "            \n",
    "            f1 = f1_score(y_test, y_pred, average='micro')\n",
    "            precision = precision_score(y_test, y_pred, average='micro')\n",
    "            recall = recall_score(y_test, y_pred, average='micro')\n",
    "            \n",
    "            metrics = {\n",
    "                'pred_duration': pred_duration,\n",
    "                'f1': f1, 'precision': precision, 'recall': recall\n",
    "            }\n",
    "            \n",
    "            with mlflow.start_run(run_name=run_name) as run:\n",
    "                mlflow.log_params(params)\n",
    "                mlflow.log_metrics(metrics)\n",
    "                mlflow.sklearn.log_model(\n",
    "                    sk_model=model, input_example=X_test, artifact_path=artifact_path\n",
    "                )\n",
    "            duration = time.time() - start\n",
    "            print(f\"run name: {run_name}.\")\n",
    "            print(metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
